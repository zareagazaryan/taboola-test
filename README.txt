=== Introduction ===

This is a simple exercise to s build a minimal end to end system (backend + database + api) that process data, saves it and serves it.
In the following sections  you will see some introduction on how to mprepare your environment and what is the exercise about.

Some notes:
* use the ide of your choice, or no ide at all :)
* please return a zipped file or a git hub project that operates on the data as expected

=== Prepare you environment ===

== Requirements ==

On Mac/Linux/Windows make sure you have these:
1. Java JDK 8
2. Maven 3+

== How to download and start the database ==

Download the database (jar file is located here https://repo1.maven.org/maven2/org/hsqldb/hsqldb/2.5.1/hsqldb-2.5.1.jar):
   cd db
   wget https://repo1.maven.org/maven2/org/hsqldb/hsqldb/2.5.1/hsqldb-2.5.1.jar

Start the database:
   cd db
   java -cp ./hsqldb-2.5.1.jar org.hsqldb.server.Server --database.0 file:test/db --dbname.0 xdb

Database JDBC connection parameters:
   driver=org.hsqldb.jdbc.JDBCDriver
   url=jdbc:hsqldb:hsql://localhost/xdb
   username=sa
   password=

Notes:
* out of the box the db is empty, so you need to connect to it using your preferred DB app and create schema as needed
* there are two scripts in db folder (download.sh, start.sh) to automate steps described here

== How to start the Api App ==

Start the api:
   cd api
   mvn clean compile spring-boot:run

Notes:
* you can test if api is running by using: curl http://localhost:8080/api/currentTime

== How to start the the Spark App ==

Start the spark stream:
   cd spark
   mvn clean compile exec:java -Dexec.mainClass="com.taboola.spark.SparkApp"

Notes:
* out of the box the stream is printing to the console all the events it is receiving every 10 seconds
* events are randomly generated by the SparkApp.getEvents function

=== The Exercise ===

Code and run an environment composed of 3 parts:
(1) a database (HsqlDB)
(2) a data ingestion pipeline (Apache Spark structured stream)
(3) a rest frontend (Spring Boot)

The spark stream continuously receives messages. Each message has 2 fields:
* timestamp
* event id (valid values: from 0 to 99)

The spark stream should collect in the database, for each time bucket and event id, a counter of all the messages received.
The time bucket has a granularity of 1 minute.

The rest frontend should have 2 endpoints:
* /api/counters/time/{time bucket}
* /api/counters/time/{time bucket}/eventId/{event id}

The first endpoint should return the counters for all event ids in a given time bucket.
The second endpoint should return the counter of events for a given time bucket and event id.

Notes:
* code must be in Java
* feel free to chose the time representation you prefer
* you can accumulate counters data in the stream or in the database, your choice
* code should deal with out of order / delayed messages
* endpoint output should be json
* warning, make sure writes to the db are performed in the spark executor, not in the driver!
* it is worth remembering that the Spark Stream runs forever: code accordingly
* as a final point, keep performance in mind: while this exercise is a small pipeline, you should code it as if it has to scale to millions of events a minute

== Example ==

Messages:
* time=March 4 2019, 10:02:58am; eventId=3
* time=March 4 2019, 10:00:01am; eventId=1
* time=March 4 2019, 10:00:10am; eventId=1
* time=March 4 2019, 10:01:30am; eventId=1
* time=March 4 2019, 10:00:05am; eventId=2
* time=March 4 2019, 10:01:35am; eventId=2
* time=March 4 2019, 10:01:36am; eventId=3
* time=March 4 2019, 10:02:22am; eventId=3

Database:
+--------------------------+----------+-------+
| time_bucket              | event_id | count |
+--------------------------+----------+-------+
| March 4 2019, 10:00:00am | 1        | 2     |
+--------------------------+----------+-------+
| March 4 2019, 10:00:00am | 2        | 1     |
+--------------------------+----------+-------+
| March 4 2019, 10:01:00am | 1        | 1     |
+--------------------------+----------+-------+
| March 4 2019, 10:01:00am | 2        | 1     |
+--------------------------+----------+-------+
| March 4 2019, 10:01:00am | 3        | 1     |
+--------------------------+----------+-------+
| March 4 2019, 10:02:00am | 3        | 2     |
+--------------------------+----------+-------+

Endpoint output:

/api/counters/time/201903041000:
{
    "1": 2,
    "2": 1
}

/api/counters/time/201903041001:
{
    "1": 1,
    "2": 1,
    "3": 1
}

/api/counters/time/201903041002:
{
    "3": 2
}

/api/counters/time/201903041001/eventId/3:
1

/api/counters/time/201903041001/eventId/99:
0
